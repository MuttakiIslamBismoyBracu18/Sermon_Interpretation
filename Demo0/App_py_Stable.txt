from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import os
import uuid
import pandas as pd
import moviepy.editor as mp
import torchaudio
import torchaudio.transforms as T
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline
import torch
import cv2
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from deepface import DeepFace
import threading
from moviepy.editor import VideoFileClip
from tensorflow.keras.models import load_model
import tensorflow as tf
import numpy as np

app = Flask(__name__)
CORS(app)

# Directories
UPLOAD_FOLDER = 'uploads'
GRAPH_FOLDER = 'graphs'
AUDIO_FOLDER = 'audios'
FRAME_FOLDER = 'frames'
DATA_FOLDER = 'data'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(GRAPH_FOLDER, exist_ok=True)
os.makedirs(AUDIO_FOLDER, exist_ok=True)
os.makedirs(FRAME_FOLDER, exist_ok=True)
os.makedirs(DATA_FOLDER, exist_ok=True)

progress_status = {"step": "", "progress": 0}
progress_lock = threading.Lock()

# Load Dress Classification Model
dress_model_path = r"E:\PRTS\formal_informal_classifier.h5"
try:
    dress_model = tf.keras.models.load_model(dress_model_path, compile=False)
    print("Model loaded successfully!")
except Exception as e:
    print(f"Error loading model: {e}")

# Load ASR & Emotion Recognition Models
asr_processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
asr_model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

audio_emotion_model = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", top_k=1)
text_emotion_model = pipeline("text-classification", model="SamLowe/roberta-base-go_emotions", top_k=1)

# Emotion Mapping
emotion_labels = ['neutral', 'angry', 'happy', 'sad', 'surprise', 'fear']
emotion_to_num = {emotion: idx for idx, emotion in enumerate(emotion_labels)}

emotion_mapping = {
    "anger": "angry",
    "disgust": "angry",
    "fear": "surprise",  
    "sadness": "sad",
    "joy": "happy",
    "optimism": "happy",
    "love": "happy",
    "surprise": "surprise",
    "neutral": "neutral",
    "trust": "neutral"
}

def map_emotion(emotion):
    """Maps detected emotion to standardized categories."""
    return emotion_mapping.get(emotion.lower(), "neutral")

# ---------------------- Audio Processing ---------------------- #
def extract_audio(video_path):
    """Extracts the audio from a video file."""
    video = mp.VideoFileClip(video_path)
    audio_path = os.path.join(AUDIO_FOLDER, f"{uuid.uuid4()}.wav")
    video.audio.write_audiofile(audio_path)
    return audio_path

def transcribe_audio(audio_path):
    """Transcribes audio into timestamped text with 1-second chunks."""
    signal, sample_rate = torchaudio.load(audio_path)

    if signal.shape[0] > 1:
        signal = torch.mean(signal, dim=0, keepdim=True)

    if sample_rate != 16000:
        resampler = T.Resample(orig_freq=sample_rate, new_freq=16000)
        signal = resampler(signal)

    chunk_duration = 1.0
    chunk_samples = int(16000 * chunk_duration)
    num_chunks = len(signal[0]) // chunk_samples

    transcript_list = []
    for i in range(num_chunks):
        chunk = signal[:, i * chunk_samples: (i + 1) * chunk_samples]
        input_values = asr_processor(chunk.squeeze(0), return_tensors="pt", sampling_rate=16000).input_values

        with torch.no_grad():
            logits = asr_model(input_values).logits

        predicted_ids = torch.argmax(logits, dim=-1)
        text = asr_processor.decode(predicted_ids[0])

        transcript_list.append({"Time": i, "Text": text})

    return transcript_list

# ---------------------- Emotion Processing ---------------------- #
def classify_audio_emotions(text):
    """Classifies emotions for the given audio transcription."""
    if text.strip():
        emotions = audio_emotion_model(text)
        detected_emotion = emotions[0][0]['label']
        return map_emotion(detected_emotion)
    return "neutral"

def classify_text_emotions(text):
    """Classifies emotions for the given text."""
    if text.strip():
        emotions = text_emotion_model(text)
        detected_emotion = emotions[0][0]['label']
        return map_emotion(detected_emotion)
    return "neutral"

# ---------------------- Video Processing ---------------------- #
def process_video(video_path):
    """Extracts emotions from video frames using DeepFace."""
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = 0
    video_emotions = []
    frames = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % int(fps) == 0:
            try:
                result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)
                dominant_emotion = result[0]['dominant_emotion']
                video_emotions.append({"Time": round(frame_count / fps, 2), "Video Emotion": dominant_emotion})

                frame_filename = f"{FRAME_FOLDER}/{uuid.uuid4()}.jpg"
                cv2.imwrite(frame_filename, frame)
                frames.append(frame_filename)
                
            except Exception as e:
                print(f"Error processing frame {frame_count}: {e}")

        frame_count += 1

    cap.release()
    return video_emotions, frames

# ---------------------- Graph & CSV Generation ---------------------- #
def generate_filtered_csvs(df, base_filename):
    matching_df = df[(df['Video Emotion'] == df['Audio Emotion']) & (df['Audio Emotion'] == df['Text Emotion'])]
    mismatching_df = df[(df['Video Emotion'] != df['Audio Emotion']) | (df['Audio Emotion'] != df['Text Emotion'])]

    matching_csv = os.path.join(DATA_FOLDER, f"{base_filename}_matching.csv")
    mismatching_csv = os.path.join(DATA_FOLDER, f"{base_filename}_mismatching.csv")

    matching_df.to_csv(matching_csv, index=False)
    mismatching_df.to_csv(mismatching_csv, index=False)

    return matching_csv, mismatching_csv


def generate_emotion_graph(df, graph_filename, filter_type="all"):
    plt.figure(figsize=(15, 6))

    if df.empty:
        print("Dataframe is empty. No data to plot.")
        return None

    colors = {'Audio Emotion': 'blue', 'Video Emotion': 'green', 'Text Emotion': 'red'}
    markers = {'Audio Emotion': 'o', 'Video Emotion': '^', 'Text Emotion': 's'}

    if filter_type == "all":
        for column in ['Audio Emotion', 'Video Emotion', 'Text Emotion']:
            emotion_nums = df[column].map(lambda x: emotion_to_num.get(x, -1)).dropna()
            if emotion_nums.empty:
                continue
            time_values = df['Time'].loc[emotion_nums.index]
            plt.plot(time_values, emotion_nums, color=colors[column], marker=markers[column], label=column)

    elif filter_type == "highlight":
        mismatching_df = df[(df['Video Emotion'] != df['Audio Emotion']) |
                            (df['Audio Emotion'] != df['Text Emotion']) |
                            (df['Video Emotion'] != df['Text Emotion'])]
        for column in ['Audio Emotion', 'Video Emotion', 'Text Emotion']:
            emotion_nums = mismatching_df[column].map(lambda x: emotion_to_num.get(x, -1)).dropna()
            time_values = mismatching_df['Time'].loc[emotion_nums.index]
            plt.scatter(time_values, emotion_nums, color=colors[column], marker=markers[column], label=column)

    elif filter_type == "fade":
        matching_df = df[(df['Video Emotion'] == df['Audio Emotion']) &
                         (df['Audio Emotion'] == df['Text Emotion'])]
        for column in ['Audio Emotion', 'Video Emotion', 'Text Emotion']:
            emotion_nums = matching_df[column].map(lambda x: emotion_to_num.get(x, -1)).dropna()
            time_values = matching_df['Time'].loc[emotion_nums.index]
            plt.scatter(time_values, emotion_nums, color=colors[column], marker=markers[column], label=column)

    elif filter_type == "sudden":
        for column in ['Audio Emotion', 'Video Emotion', 'Text Emotion']:
            non_neutral_df = df[df[column] != 'neutral']
            if not non_neutral_df.empty:
                emotion_nums = non_neutral_df[column].map(lambda x: emotion_to_num.get(x, -1))
                time_values = non_neutral_df['Time']
                plt.plot(
                    time_values,
                    emotion_nums,
                    color=colors[column],
                    marker=markers[column],
                    label=f"{column} (Non-Neutral)",
                )
    plt.yticks(range(len(emotion_labels)), emotion_labels)
    plt.xlabel("Time (seconds)")
    plt.ylabel("Emotion")
    plt.title(f"Emotion Trends Over Time ({filter_type.capitalize()} Graph)")
    plt.legend()
    plt.grid(True)

    graph_path = os.path.join(GRAPH_FOLDER, graph_filename)
    plt.savefig(graph_path)
    plt.close()
    return graph_path

# ---------------------- Find Matching Emotions ---------------------- #
def find_matching_emotions(df):
    """Finds times where Video, Audio, and Text emotions match."""
    matching_times = df[(df['Video Emotion'] == df['Audio Emotion']) & (df['Audio Emotion'] == df['Text Emotion'])]
    return matching_times[['Time', 'Video Emotion']]

# ---------------------- Dress Code ---------------------- #
def preprocess_frame(frame):
    frame = cv2.resize(frame, (224, 224))  
    frame = frame / 255.0  # Normalize
    frame = np.expand_dims(frame, axis=0)  
    return frame

def classify_dress(video_path):
    cap = cv2.VideoCapture(video_path)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    dress_predictions = []

    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_count % fps == 0:  
            processed_frame = preprocess_frame(frame)
            prediction = dress_model.predict(processed_frame)
            dress_predictions.append(prediction[0][0])

        frame_count += 1
    cap.release()

    avg_score = np.mean(dress_predictions)
    return "Formal" if avg_score >= 0.31 else "Informal"

# ---------------------- Routes ---------------------- #
@app.route('/upload', methods=['POST'])
def upload_video():
    try:
        file = request.files['file']
        update_progress("Uploading video file...", 10)

        video_path = os.path.join(UPLOAD_FOLDER, file.filename)
        file.save(video_path)

        dress_code = classify_dress(video_path)
        print(f"🔹 Detected Dress Code: {dress_code}")

        update_progress("Processing video frames for emotions...", 20)
        video_emotions, frames = process_video(video_path)

        update_progress("Extracting audio from video...", 40)
        audio_path = extract_audio(video_path)

        update_progress("Transcribing audio to text...", 60)
        transcript_segments = transcribe_audio(audio_path)

        if not transcript_segments:
            update_progress("Audio transcription failed.", 100)
            return jsonify({"error": "Audio transcription failed. No text was generated."}), 500

        update_progress("Classifying emotions for audio and text...", 80)
        audio_emotions = [{"Time": t["Time"], "Audio Emotion": classify_audio_emotions(t.get("Text", ""))} for t in transcript_segments]
        text_emotions = [{"Time": t["Time"], "Text Emotion": classify_text_emotions(t.get("Text", ""))} for t in transcript_segments]

        update_progress("Merging data and generating CSV...", 90)
        df = pd.DataFrame(transcript_segments).merge(pd.DataFrame(video_emotions), on="Time", how="outer") \
                                              .merge(pd.DataFrame(audio_emotions), on="Time", how="outer") \
                                              .merge(pd.DataFrame(text_emotions), on="Time", how="outer") \
                                              .fillna(method="ffill")

        base_filename = str(uuid.uuid4())
        csv_filename = f"{base_filename}_emotions.csv"
        csv_path = os.path.join(DATA_FOLDER, csv_filename)

        df = df[['Time', 'Audio Emotion', 'Video Emotion', 'Text Emotion']]
        df.to_csv(csv_path, index=False)

        update_progress("Generating emotion graphs...", 95)
        generate_filtered_csvs(df, base_filename)
        graph_filenames = {}
        for filter_type in ['all', 'highlight', 'fade', 'sudden']:
            graph_filename = f"{filter_type}_graph.png"
            graph_path = generate_emotion_graph(df, graph_filename, filter_type)
            graph_filenames[filter_type] = graph_filename

        transcript_text = "\n".join([f"{seg['Time']}s: {seg.get('Text', 'N/A')}" for seg in transcript_segments if seg.get("Text")])

        update_progress("Processing complete!", 100)
        return jsonify({
            "dress_code": dress_code,
            "frames": frames,
            "graphs": graph_filenames,
            "csv": csv_filename,
            "transcript": transcript_text,
            "video_emotions": df.to_dict(orient="records")
        })
    except Exception as e:
        print(f"Error: {e}")
        update_progress("An error occurred.", 100)
        return jsonify({"error": "An internal server error occurred."}), 500



@app.route('/matching_emotions', methods=['GET'])
def get_matching_emotions():
    """Returns times where all three emotions match."""
    csv_files = sorted(os.listdir(DATA_FOLDER), reverse=True)
    if not csv_files:
        return jsonify({"error": "No emotion data available."}), 404
    
    latest_csv = os.path.join(DATA_FOLDER, csv_files[0])
    df = pd.read_csv(latest_csv)
    matching_emotions = find_matching_emotions(df)
    return jsonify(matching_emotions.to_dict(orient='records'))

@app.route('/graphs/<path:filename>')
def serve_graph(filename):
    return send_from_directory(GRAPH_FOLDER, filename)

@app.route('/download_csv/<filename>')
def download_csv(filename):
    return send_from_directory(DATA_FOLDER, filename, as_attachment=True)

@app.route('/generate_graphs', methods=['GET'])
def generate_all_graphs():
    csv_files = sorted(os.listdir(DATA_FOLDER), reverse=True)
    if not csv_files:
        return jsonify({"error": "No emotion data available."}), 404

    latest_csv = os.path.join(DATA_FOLDER, csv_files[0])
    df = pd.read_csv(latest_csv)

    graph_filenames = {}
    for filter_type in ['all', 'highlight', 'fade', 'sudden']:
        graph_filename = f"{filter_type}_graph.png"
        graph_path = generate_emotion_graph(df, graph_filename, filter_type)
        graph_filenames[filter_type] = graph_filename 

    print("Generated graphs:", graph_filenames) 
    return jsonify(graph_filenames)

# ---------------------- Progress ---------------------- #
def update_progress(step, progress):
    with progress_lock:
        progress_status["step"] = step
        progress_status["progress"] = progress

@app.route('/progress', methods=['GET'])
def get_progress():
    with progress_lock:
        return jsonify(progress_status)

if __name__ == '__main__':
    app.run(debug=True)