# -*- coding: utf-8 -*-
"""Emotion Detection with Time.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rse4uzEPUdfTAZSyYT2OwAXMkf0rz2XC

# LibriSpeech Dataset
"""

!wget -q https://www.openslr.org/resources/12/dev-clean.tar.gz
!tar -xvzf dev-clean.tar.gz --wildcards --no-anchored "*.flac" -C /content/
audio_file_path = "/content/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac"

"""# Observe the Audio Shape"""

import torchaudio

signal, sample_rate = torchaudio.load('/content/LibriSpeech/dev-clean/422/122949/422-122949-0000.flac')
print(f"Shape of audio signal: {signal.shape}")
print(f"Sample rate: {sample_rate}")

"""# Convert Audio Shape to 16Hz"""

!pip install torchaudio scipy


import torchaudio
import torch
from scipy.io.wavfile import write
import os


def download_sample_audio():
    url = "https://www2.cs.uic.edu/~i101/SoundFiles/preamble10.wav"
    audio_path = "sample_audio.wav"
    if not os.path.exists(audio_path):
        !wget -O {audio_path} {url}
    return audio_path


def convert_audio(input_path, output_path, desired_duration=5.0, desired_sample_rate=16000):

    if not os.path.exists(input_path):
        raise FileNotFoundError(f"The specified audio file does not exist: {input_path}")

    signal, sample_rate = torchaudio.load(input_path)

    if sample_rate != desired_sample_rate:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=desired_sample_rate)
        signal = resampler(signal)
        sample_rate = desired_sample_rate

    if signal.shape[0] > 1:
        signal = signal.mean(dim=0, keepdim=True)

    num_samples = int(desired_sample_rate * desired_duration)
    if signal.shape[1] > num_samples:
        signal = signal[:, :num_samples]
    else:
        padding = num_samples - signal.shape[1]
        signal = torch.nn.functional.pad(signal, (0, padding))

    signal = signal.squeeze().unsqueeze(0)

    signal_np = signal.numpy()
    write(output_path, sample_rate, signal_np[0])

    print(f"Audio saved to: {output_path}")
    return output_path


input_audio_path = download_sample_audio()
output_audio_path = "converted_audio.wav"
converted_audio_path = convert_audio(input_audio_path, output_audio_path, desired_duration=5.0, desired_sample_rate=16000)

"""# Speech to Emotion with Time Interval"""

!pip install transformers torchaudio gradio

from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline
import torch
import torchaudio
import gradio as gr

asr_processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
asr_model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

emotion_recognition = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=True)

def predict_emotion_from_audio(audio, chunk_duration=1.0):
    try:
        signal, fs = torchaudio.load(audio)

        if fs != 16000:
            resampler = torchaudio.transforms.Resample(orig_freq=fs, new_freq=16000)
            signal = resampler(signal)

        if signal.shape[0] > 1:
            signal = signal.mean(dim=0)


        signal = signal.unsqueeze(0) if signal.ndim == 1 else signal
        if signal.ndim == 3:
            signal = signal.squeeze(0)

        chunk_samples = int(16000 * chunk_duration)
        num_chunks = signal.shape[1] // chunk_samples

        time_emotions = []

        for i in range(num_chunks):
            chunk = signal[:, i * chunk_samples : (i + 1) * chunk_samples]

            chunk = chunk.squeeze(0) if chunk.dim() == 2 else chunk

            input_values = asr_processor(chunk, sampling_rate=16000, return_tensors="pt").input_values
            with torch.no_grad():
                logits = asr_model(input_values).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcription = asr_processor.decode(predicted_ids[0])

            emotions = emotion_recognition(transcription)

            dominant_emotion = max(emotions[0], key=lambda x: x['score'])

            time_emotions.append((i * chunk_duration, dominant_emotion['label'], dominant_emotion['score']))

        remaining_chunk = signal[:, num_chunks * chunk_samples:]
        if remaining_chunk.shape[1] > 0:
            remaining_chunk = remaining_chunk.squeeze(0) if remaining_chunk.dim() == 2 else remaining_chunk
            input_values = asr_processor(remaining_chunk, sampling_rate=16000, return_tensors="pt").input_values
            with torch.no_grad():
                logits = asr_model(input_values).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcription = asr_processor.decode(predicted_ids[0])

            emotions = emotion_recognition(transcription)
            dominant_emotion = max(emotions[0], key=lambda x: x['score'])
            time_emotions.append((num_chunks * chunk_duration, dominant_emotion['label'], dominant_emotion['score']))

        output = "Time vs Dominant Emotion:\n\n"
        for time, emotion, score in time_emotions:
            output += f"Time {time:.1f}s: Emotion: {emotion}, Score: {score:.2f}\n"

        return output

    except OSError as e:
        return f"File error: {e}"
    except AttributeError as e:
        return f"Model or processing error: {e}"
    except Exception as e:
        return f"An unexpected error occurred: {e}"

iface = gr.Interface(
    fn=predict_emotion_from_audio,
    inputs=gr.Audio(type="filepath"),
    outputs="text",
    title="Emotion Detection from Speech",
    description="Upload an audio file to detect dominant emotion for each second of the speech."
)

iface.launch()